---
title: "Clustering Project: Quiz Prosperous Couple Leads Analysis"
author: "Gabriel Ferreira"
date: "2025-06-04"
format:
  html:
    page-layout: full
    code-fold: true
    code-summary: "Show Code"
    toc: false
    anchor-sections: false
engine: jupytern
jupyter: python3
---
# Introduction

Analyze data collected through an interactive quiz, focused on couples planning their wedding. The main purpose is to segment these leads to better understand their profiles, needs, and stage in planning.

The main objective was to use unsupervised learning techniques (clustering) to identify distinct groups of couples based on their responses, allowing for the creation of more effective and personalized marketing strategies.

# Project Structure

The development of this project followed a structured and iterative methodology, covering everything from data collection and preparation to the evaluation and interpretation of results.

## Data Collection and Preparation

The initial phase involved the consolidation and cleaning of data. Leads were collected from three distinct sources, requiring a unification and standardization process to ensure dataset quality.

### Data Loading and Concatenation

- Data Source: Collection of online quiz responses, distributed across 03 CSV files.
- Loading and Unification: Reading different files and consolidating them into a single dataset.

```{python}
#| include: false
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.subplots as sp
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express.colors as pcolors
from plotly.colors import qualitative
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from scipy.stats import chi2_contingency
import warnings
warnings.filterwarnings('ignore')
```

```{python}
#| include: false
caminho_do_df1 = '../dados/leads_funil-casamento_df1.csv'
caminho_do_df2 = '../dados/leads_funil-casamento_df2.csv'
caminho_do_df3 = '../dados/leads_funil-casamento_df3.csv'
df_01 = pd.read_csv(caminho_do_df1)
df_02 = pd.read_csv(caminho_do_df2)
df_03 = pd.read_csv(caminho_do_df3)
```
###### First 5 rows of df01:
```{python}
df_01.head(5)
```

###### Count of each column df01:
```{python}
df_01.count()
```

###### Count of each column df02:
```{python}
df_02.count()
```


### Data Cleaning

The columns created_at options: opcoes_UAa9JQ, code,button: oQcJxV, button: 0FvZw6, button: enviar and tracking do not contain relevant information for the analysis, so we can remove them from df01 and df02. The columns field: 11ABFZ, field: 3hxWeL, field: 5fSLVc contain data about the lead, but have too little data to work with, so we will also remove them from the dataframes..

###### Removal of defined columns from df01 and df02:
```{python}
df_01 = df_01.drop(columns=["created_at","options: opcoes_UAa9JQ","code", "button: oQcJxV", "button: 0FvZw6", "field: 11ABFZ", "field: 3hxWeL","field: 5fSLVc", "button: enviar", "tracking"])
df_02 = df_02.drop(columns=["created_at","options: opcoes_UAa9JQ","code", "button: oQcJxV", "button: 0FvZw6", "field: 11ABFZ", "field: 3hxWeL","field: 5fSLVc", "button: enviar", "tracking"])
```

Let's remove the columns:
code
button: oQcJxV
options: opcoes_UAa9JQ
button: 0FvZw6
options: opcoes_PhNxWH
options: opcoes_NMBS1J
options: opcoes_Peavhn
options: opcoes_RyUh7O
options: opcoes_S8x7OR
options: opcoes_MNd05q
options: opcoes_AQo3UU
options: opcoes_yYueg1
field: 11ABFZ
field: 3hxWeL
field: 5fSLVc
button: enviar
tracking

from df_03, for the same reasons as the removal of columns from df1 and df02.

###### Removal of defined columns from df03:
```{python}
df_03 = df_03.drop(columns=["created_at","code", "button: oQcJxV","options: opcoes_UAa9JQ","button: 0FvZw6",
                            "button: 0FvZw6", "options: opcoes_PhNxWH", "options: opcoes_NMBS1J", "options: opcoes_Peavhn",
                            "options: opcoes_RyUh7O", "options: opcoes_S8x7OR", "options: opcoes_MNd05q", "options: opcoes_AQo3UU",
                            "options: opcoes_yYueg1","field: 11ABFZ", "field: 3hxWeL","field: 5fSLVc", "button: enviar", "tracking"])
```

Now all three dataframes have the same columns, let's rename them and concatenate the dataframes.

###### Column Renaming and Concatenating the dataframes:
```{python}
rename_dict = {
    'options: opcoes_KxEbYn': 'pergunta_1',
    'options: opcoes_iSdQa9': 'pergunta_2',
    'options: opcoes_GZPJo1': 'pergunta_3',
    'options: opcoes_n5nnNG': 'pergunta_4',
    'options: opcoes_G7CtE4': 'pergunta_5',
    'options: opcoes_euOjgt': 'pergunta_6',
    'options: opcoes_3FPmNX': 'pergunta_7',
    'options: opcoes_kGxmr1': 'pergunta_8'
}
# Renomear colunas para df_01
df_01 = df_01.rename(columns=rename_dict)
df_02 = df_02.rename(columns=rename_dict)
df_03 = df_03.rename(columns=rename_dict)
# Concatenar os dataframes
df_gf = pd.concat([df_01, df_02, df_03], ignore_index=True)
```

###### Sample of the cleaned and concatenated dataframe:
```{python}
df_gf.sample(3)
```

We noticed that some answers have the letter A/B/C/D before the answer and other answers do not. We need to map both scenarios.

###### Mapping responses (with and without letter):
```{python}
# Dictionary to map responses with letter
mapeamento_com_letra = {
    '(A) Ainda estamos completamente perdidos sobre tudo': 'A',
    '(B) Temos algumas referÃªncias, mas nada decidido': 'B',
    '(C) JÃ¡ temos o estilo em mente, mas falta planejar': 'C',
    '(D) Sabemos exatamente o que queremos e jÃ¡ comeÃ§amos a organizar': 'D',

    '(A) Vivemos no limite e temos dÃ­vidas': 'A',
    '(B) Conseguimos nos manter, mas nÃ£o sobra': 'B',
    '(C) Temos folga, mas ainda nÃ£o vivemos como gostarÃ­amos': 'C',
    '(D) Estamos bem financeiramente, mas queremos crescer mais': 'D',

    '(A) EstÃ¡ completamente envolvido(a), sonha junto comigo': 'A',
    '(B) EstÃ¡ envolvido(a), mas prefere que eu lidere': 'B',
    '(C) Me apoia, mas nÃ£o se envolve muito com o planejamento': 'C',
    '(D) Prefere que eu resolva tudo sozinho(a)': 'D',

    '(A) NÃ£o comeÃ§amos ainda': 'A',
    '(B) Temos anotaÃ§Ãµes e ideias soltas': 'B',
    '(C) Criamos um planejamento inicial, mas ainda sem orÃ§amento': 'C',
    '(D) Temos planilhas, metas e atÃ© cronograma definido': 'D',

    '(A) NÃ£o conseguirÃ­amos bancar nada ainda': 'A',
    '(B) ConseguirÃ­amos fazer algo simples': 'B',
    '(C) TerÃ­amos que parcelar bastante ou contar com ajuda': 'C',
    '(D) PoderÃ­amos arcar com boa parte, mas queremos mais liberdade': 'D',

    '(A) Ainda nÃ£o pensamos nisso': 'A',
    '(B) Algo Ã­ntimo e simples, sÃ³ com pessoas prÃ³ximas': 'B',
    '(C) Uma cerimÃ´nia encantadora, com tudo bem feito': 'C',
    '(D) Um evento inesquecÃ­vel, com tudo que temos direito': 'D',

    '(A) Nem pensamos nisso ainda': 'A',
    '(B) Pensamos, mas parece fora da nossa realidade': 'B',
    '(C) Temos destinos em mente, mas sem orÃ§amento ainda': 'C',
    '(D) JÃ¡ sabemos onde queremos ir e estamos nos planejando': 'D',

    '(A) Desejamos, mas nos falta tempo e direÃ§Ã£o': 'A',
    '(B) Queremos muito, mas temos medo de nÃ£o dar conta': 'B',
    '(C) Estamos dispostos, sÃ³ falta um plano eficaz': 'C',
    '(D) Estamos prontos, queremos agir e realizar de verdade': 'D'
}
# Dictionary to map responses without letter
mapeamento_sem_letra = {
    'Ainda estamos completamente perdidos sobre tudo': 'A',
    'Temos algumas referÃªncias, mas nada decidido': 'B',
    'JÃ¡ temos o estilo em mente, mas falta planejar': 'C',
    'Sabemos exatamente o que queremos e jÃ¡ comeÃ§amos a organizar': 'D',

    'Vivemos no limite e temos dÃ­vidas': 'A',
    'Conseguimos nos manter, mas nÃ£o sobra': 'B',
    'Temos folga, mas ainda nÃ£o vivemos como gostarÃ­amos': 'C',
    'Estamos bem financeiramente, mas queremos crescer mais': 'D',

    'EstÃ¡ completamente envolvido(a), sonha junto comigo': 'A',
    'EstÃ¡ envolvido(a), mas prefere que eu lidere': 'B',
    'Me apoia, mas nÃ£o se envolve muito com o planejamento': 'C',
    'Prefere que eu resolva tudo sozinho(a)': 'D',

    'NÃ£o comeÃ§amos ainda': 'A',
    'Temos anotaÃ§Ãµes e ideias soltas': 'B',
    'Criamos um planejamento inicial, mas ainda sem orÃ§amento': 'C',
    'Temos planilhas, metas e atÃ© cronograma definido': 'D',

    'NÃ£o conseguirÃ­amos bancar nada ainda': 'A',
    'ConseguirÃ­amos fazer algo simples': 'B',
    'TerÃ­amos que parcelar bastante ou contar com ajuda': 'C',
    'PoderÃ­amos arcar com boa parte, mas queremos mais liberdade': 'D',

    'Ainda nÃ£o pensamos nisso': 'A',
    'Algo Ã­ntimo e simples, sÃ³ com pessoas prÃ³ximas': 'B',
    'Uma cerimÃ´nia encantadora, com tudo bem feito': 'C',
    'Um evento inesquecÃ­vel, com tudo que temos direito': 'D',

    'Nem pensamos nisso ainda': 'A',
    'Pensamos, mas parece fora da nossa realidade': 'B',
    'Temos destinos em mente, mas sem orÃ§amento ainda': 'C',
    'JÃ¡ sabemos onde queremos ir e estamos nos planejando': 'D',

    'Desejamos, mas nos falta tempo e direÃ§Ã£o': 'A',
    'Queremos muito, mas temos medo de nÃ£o dar conta': 'B',
    'Estamos dispostos, sÃ³ falta um plano eficaz': 'C',
    'Estamos prontos, queremos agir e realizar de verdade': 'D'
}
# Function to map responses
def mapear_resposta(resposta, mapeamento_letra, mapeamento_sem):
    if isinstance(resposta, str):
        resposta = resposta.strip()
    
    resposta_mapeada = mapeamento_letra.get(resposta)
    if resposta_mapeada:
        return resposta_mapeada
    
    resposta_mapeada = mapeamento_sem.get(resposta)
    if resposta_mapeada:
        return resposta_mapeada
# Applies the function to all question columns
colunas_perguntas = ['pergunta_1', 'pergunta_2', 'pergunta_3', 'pergunta_4', 'pergunta_5', 'pergunta_6', 'pergunta_7', 'pergunta_8']
df_gf[colunas_perguntas] = df_gf[colunas_perguntas].applymap(lambda resposta: mapear_resposta(resposta, mapeamento_com_letra, mapeamento_sem_letra))
```

### Handling Missing Values

###### Describe of the dataframe:
```{python}
df_gf.describe()
```

###### Checking for missing values:
```{python}
print(df_gf.isnull().sum())
```

Since we have a lot of NaN values, we decided to remove all rows that have all questions unanswered and then replace the rows that have NaN but not in all questions with the mode of each question.

###### Removing rows with null values:
```{python}
# Contar diretamente as linhas onde todas as colunas sÃ£o NaN
num_linhas_todas_nan = df_gf.isna().all(axis=1).sum()
# Remover linhas onde todas as colunas sÃ£o NaN
df_gf= df_gf.dropna(how='all')
# Verificando valores nulos novamente
print(df_gf.isnull().sum())
```

Now let's analyze the data to find the volume of missing values per question and how the answers are distributed for each question.

###### Calculating the percentage of missing values in each column:
```{python}
# Calculando o percentual de valores ausentes em cada coluna
percentual_na_perguntas = df_gf.isna().mean() * 100
# Exibindo o percentual de valores ausentes
print(percentual_na_perguntas)
```

Before cleaning the nan values, let's look at how the data is distributed.

###### Creating chart without null values:
```{python}
# Criando um df sem os valores nan
df_limpo = df_gf.dropna()
# Lista das perguntas
perguntas = ['pergunta_1', 'pergunta_2', 'pergunta_3', 'pergunta_4', 
             'pergunta_5', 'pergunta_6', 'pergunta_7', 'pergunta_8']
# Cores do grÃ¡fico
cores = px.colors.qualitative.Vivid
# Criando o grÃ¡fico com subplots
fig = sp.make_subplots(
    rows=2, cols=4, 
    subplot_titles=perguntas,
    horizontal_spacing=0.10,
    vertical_spacing=0.13
)
for i, pergunta in enumerate(perguntas):
    contagem = df_limpo[pergunta].value_counts().sort_index()
    
    fig.add_trace(
        go.Bar(
            x=contagem.index,
            y=contagem.values,
            marker_color=cores[:len(contagem)],
            name=pergunta
        ),
        row=(i//4)+1, col=(i%4)+1
    )
fig.update_layout(
    height=500,
    width=900,
    title={
        'text': "Frequency of Answers per Question",
        'y': 0.98,          
        'x': 0.5,           
        'xanchor': 'center',
        'yanchor': 'top',
        'pad': {'b': 28}
    },
    margin=dict(t=95, b=60, l=40, r=40),
    showlegend=False,
    template="plotly_white"
)
fig.show()
```

Since we currently have a low percentage of null values in the dataset, we will replace the null values with the Mode of each question.

###### Imputing null values with the mode of each column:
```{python}
for col in df_gf.columns:
    if col.startswith('pergunta'):
        moda = df_gf[col].mode()[0]
        df_gf[col].fillna(moda, inplace=True)
```

###### Checking for null values again:
```{python}
print(df_gf.isnull().sum())
```

Let's create a data dictionary in case it's necessary to consult the questions and alternatives during the analysis.

###### Creating a dictionary with all questions and alternatives:
```{python}
perguntas_dict = {
    "pergunta_1": {
        "texto": "NÃ­vel de clareza sobre o casamento dos sonhos: Como vocÃªs descreveriam o nÃ­vel de clareza que tÃªm sobre o casamento que desejam?",
        "alternativas": {
            "A": "Ainda estamos completamente perdidos sobre tudo",
            "B": "Temos algumas referÃªncias, mas nada decidido",
            "C": "JÃ¡ temos o estilo em mente, mas falta planejar",
            "D": "Sabemos exatamente o que queremos e jÃ¡ comeÃ§amos a organizar"
        }
    },
    "pergunta_2": {
        "texto": "SituaÃ§Ã£o financeira atual: Como vocÃª descreveria a situaÃ§Ã£o financeira atual de vocÃªs dois?",
        "alternativas": {
            "A": "Vivemos no limite e temos dÃ­vidas",
            "B": "Conseguimos nos manter, mas nÃ£o sobra",
            "C": "Temos folga, mas ainda nÃ£o vivemos como gostarÃ­amos",
            "D": "Estamos bem financeiramente, mas queremos crescer mais"
        }
    },
    "pergunta_3": {
        "texto": "Apoio mÃºtuo e envolvimento no sonho de casamento: Como estÃ¡ o envolvimento do seu parceiro(a) na realizaÃ§Ã£o do casamento dos sonhos?",
        "alternativas": {
            "A": "EstÃ¡ completamente envolvido(a), sonha junto comigo",
            "B": "EstÃ¡ envolvido(a), mas prefere que eu lidere",
            "C": "Me apoia, mas nÃ£o se envolve muito com o planejamento",
            "D": "Prefere que eu resolva tudo sozinho(a)"
        }
    },
    "pergunta_4": {
        "texto": "NÃ­vel de organizaÃ§Ã£o do planejamento: Como vocÃªs estÃ£o se organizando para planejar o casamento?",
        "alternativas": {
            "A": "NÃ£o comeÃ§amos ainda",
            "B": "Temos anotaÃ§Ãµes e ideias soltas",
            "C": "Criamos um planejamento inicial, mas ainda sem orÃ§amento",
            "D": "Temos planilhas, metas e atÃ© cronograma definido"
        }
    },
    "pergunta_5": {
        "texto": "Possibilidade de investimento atual no casamento: Se fossem realizar o casamento ideal hoje, como pagariam?",
        "alternativas": {
            "A": "NÃ£o conseguirÃ­amos bancar nada ainda",
            "B": "ConseguirÃ­amos fazer algo simples",
            "C": "TerÃ­amos que parcelar bastante ou contar com ajuda",
            "D": "PoderÃ­amos arcar com boa parte, mas queremos mais liberdade"
        }
    },
    "pergunta_6": {
        "texto": "Estilo de casamento desejado: Qual o estilo de casamento dos seus sonhos?",
        "alternativas": {
            "A": "Ainda nÃ£o pensamos nisso",
            "B": "Algo Ã­ntimo e simples, sÃ³ com pessoas prÃ³ximas",
            "C": "Uma cerimÃ´nia encantadora, com tudo bem feito",
            "D": "Um evento inesquecÃ­vel, com tudo que temos direito"
        }
    },
    "pergunta_7": {
        "texto": "Planejamento da lua de mel: VocÃªs jÃ¡ pensaram na lua de mel?",
        "alternativas": {
            "A": "Nem pensamos nisso ainda",
            "B": "Pensamos, mas parece fora da nossa realidade",
            "C": "Temos destinos em mente, mas sem orÃ§amento ainda",
            "D": "JÃ¡ sabemos onde queremos ir e estamos nos planejando"
        }
    },
    "pergunta_8": {
        "texto": "Comprometimento em tornar esse sonho realidade: O quanto vocÃªs estÃ£o comprometidos em transformar esse sonho em realidade?",
        "alternativas": {
            "A": "Desejamos, mas nos falta tempo e direÃ§Ã£o",
            "B": "Queremos muito, mas temos medo de nÃ£o dar conta",
            "C": "Estamos dispostos, sÃ³ falta um plano eficaz",
            "D": "Estamos prontos, queremos agir e realizar de verdade"
        }
    }
}
```

## 3. Model Building and Validation

The KMeans algorithm uses the Euclidean distance between points to form clusters. Euclidean distance is a measure of how far apart two points are in Euclidean space. Correlation can have a significant impact on clustering models like KMeans, as KMeans uses Euclidean distance between points to form clusters. Highly correlated variables can disproportionately influence the distances between points, leading to possible distortions in the formed clusters.

We need to consider multicollinearity (if two variables are highly correlated), the scale of variables (since KMeans is sensitive to scale), and dimensionality reduction (to speed up the grouping process when there are a large number of variables).

Therefore, before applying KMeans, let's first analyze the correlation of categorical data. We will use CramÃ©r's V, which is a measure of association between two categorical variables, measuring how associated two nominal variables are.

### Calculating Cramer's V Matrix and performing the analysis:
```{python}
# FunÃ§Ã£o para calcular Cramer's V
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))
    rcorr = r - ((r-1)**2) / (n-1)
    kcorr = k - ((k-1)**2) / (n-1)
    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))
# Criando matriz 
df = df_gf
categorical_columns = df.columns
# Criar uma matriz vazia
cramers_results = pd.DataFrame(np.zeros((len(categorical_columns), len(categorical_columns))),
                               index=categorical_columns,
                               columns=categorical_columns)
# Preencher a matriz
for col1 in categorical_columns:
    for col2 in categorical_columns:
        cramers_results.loc[col1, col2] = cramers_v(df[col1], df[col2])
# Imprimindo a mtriz
cramers_results
```

# Visualizing the correlation matrix:
```{python}
plt.figure(figsize=(10,8))
sns.heatmap(cramers_results, annot=True, cmap='coolwarm', vmin=0, vmax=1)
plt.title("CramÃ©r's V - Correlation between categorical variables")
plt.show()
```

We do not have highly correlated variables, but let's analyze the three largest correlations to assess the consistency of this dataset.

###### Evaluating the three largest correlations:
```{python}
# Transformar a matriz em long format (par variÃ¡vel - valor de correlaÃ§Ã£o)
corr_pairs = (
    cramers_results.where(np.triu(np.ones(cramers_results.shape), k=1).astype(bool))  # pegar sÃ³ triÃ¢ngulo superior (evita repetiÃ§Ã£o)
    .stack()  # transformar em Series com MultiIndex
    .reset_index()
)
corr_pairs.columns = ['1', '2', 'Cramers_V']
# Ordenar pelas maiores correlaÃ§Ãµes
top_corr = corr_pairs.sort_values(by='Cramers_V', ascending=False).head(3)
print(top_corr)
```

###### Analyzing correlation between questions 1 and 4:
```{python}
perguntas_dict["pergunta_1"]["texto"], perguntas_dict["pergunta_4"]["texto"]
```

This correlation makes sense because those who have clarity about the wedding they desire tend to be more organized regarding the planning.

###### Analyzing correlation between questions 4 and 8:
```{python}
perguntas_dict["pergunta_4"]["texto"], perguntas_dict["pergunta_8"]["texto"]
```

This correlation also makes sense because those who are more organized will be more committed to making the wedding happen.

###### Analyzing correlation between questions 2 and 5:
```{python}
perguntas_dict["pergunta_2"]["texto"], perguntas_dict["pergunta_5"]["texto"]
```

These questions also have a justifiable correlation, as the couple's financial situation will directly impact the possibility of current investment in the wedding

**Conclusion**

- There are no very strong relationships between the questions, which is expected in well-designed questionnaires where questions measure distinct, albeit related, aspects (no high risk of multicollinearity).
- The highest correlations are classified as moderate or weak, indicating that each question captures different aspects of the leads' profile or situation.

We will still apply PCA for dimensionality reduction, but it is not mandatory due to multicollinearity between variables, but rather because we are interested in reducing computational complexity and improving the efficiency of KNN in high-dimensional spaces.


### One-Hot-Encoding Application

Since we have many features and need to feed the algorithm with numerical variables, we will apply OHE with the drop_first=True parameter. OHE transforms categorical variables into a binary numerical format, essential for algorithms that do not natively handle categories, as is the case with KMeans.

###### Applying One-Hot Encoding with drop_first=True:
```{python}
# Instanciando o codificador
ohe = OneHotEncoder(drop='first', sparse=False)
# Ajustando e transformando os dados
ohe_array = ohe.fit_transform(df_gf)
# Pegando os nomes das colunas geradas pelo OHE
ohe_columns = ohe.get_feature_names_out(df_gf.columns)
# Criando um novo DataFrame com os dados codificados
df_gf_ohe = pd.DataFrame(ohe_array, columns=ohe_columns, index=df_gf.index)
# Visualizando amostra aleatÃ³ria de 10 linhas
df_gf_ohe.sample(10)
```

We can see the use of the drop='first' parameter, which removes the first category of each variable to reduce dimensionality; we have 24 features instead of 32.

### PCA Application

Principal Component Analysis (PCA) is a statistical dimensionality reduction technique that transforms a set of possibly correlated variables into a new set of uncorrelated variables, called principal components.

The objective for our project is to capture the maximum variability of the data in the first components and facilitate visualization, clustering, classification, and reduce noise. OHE significantly increases dimensionality (in this project's case, from 8 questions to 24 variables after drop_first=True).

###### Applying PCA
```{python}
# Padronizando os dados
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_gf_ohe)
# Instanciando o PCA
pca = PCA()
# Ajustando o PCA aos dados
pca.fit(df_scaled)
# Gerando os componentes principais
df_pca = pca.transform(df_scaled)
# Convertendo em DataFrame para visualizaÃ§Ã£o
df_pca = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(df_pca.shape[1])])
# Visualizando as 5 primeiras linhas
print(df_pca.head())
```

The result of the principal component analysis aims to provide a basis for us to decide how many variables we will use and how much total variance we can explain in this dataset. Therefore, we should choose between 01 to 24 PCs and make a decision based on "how much we want to explain from this data," i.e., the minimum components needed to have maximum interpretability.

###### Scree Plot - Explained Variance by PCA:
```{python}
# Dados da variÃ¢ncia explicada
variancia_acumulada = np.cumsum(pca.explained_variance_ratio_)
# Criando a figura
fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=list(range(1, len(variancia_acumulada) + 1)),
        y=variancia_acumulada,
        mode='lines+markers',
        line=dict(dash='dash', width=2),
        marker=dict(size=8, color='blue'),
        name='Accumulated Variance'
    )
)
fig.update_layout(
    title='Scree Plot - Explained Variance by PCA',
    xaxis_title='Number of Components',
    yaxis_title='Accumulated Explained Variance',
    template='plotly_white',
    width=800,
    height=500
)
fig.show()
```

###### Visualizing the explained variance by each component:
```{python}
for i, var in enumerate(pca.explained_variance_ratio_):
    print(f'PC{i+1}: {var:.4f} ({np.cumsum(pca.explained_variance_ratio_)[i]:.4f} accumulated)')
```

We chose to use 17 principal components, which preserve 90.31% of the total variance of the dataset, ensuring a balance between data simplification and information maintenance. This value was determined based on the analysis of the scree plot and the accumulated variance distribution, which shows the absence of a clear elbow, characteristic of categorical data. This strategy allows for faster processing, improved model performance, and maintained analytical robustness.

### Defining the Value of K in Clustering Models

The KMeans algorithm is a data clustering technique that organizes a set of points into groups (or "clusters") based on their similarities. Choosing the appropriate value of k is an important step in the project, as it can significantly affect the usefulness of the formed clusters. To do this, we will use the Elbow Method and the Silhouette Score to help define an optimal value for k.

**Defining the ideal value of K**

###### Generating Dataset with 17 principal components::
```{python}
pca = PCA(n_components=17)
df_pca = pca.fit_transform(df_gf_ohe)
```

###### Elbow Method:
```{python}
inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_pca)
    inertia.append(kmeans.inertia_)
# Plot do Elbow
fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=list(K_range),
        y=inertia,
        mode='lines+markers',
        marker=dict(size=8, color='blue'),
        line=dict(width=2),
        name='Inertia'
    )
)
fig.update_layout(
    title='Elbow Method',
    xaxis_title='Number of Clusters (K)',
    yaxis_title='Inertia',
    template='plotly_white',
    width=800,
    height=500
)
fig.show()
```

Interpretation: evaluates the sum of squared distances within clusters (Sum of Squared Errors - SSE) as a function of different k values. As k increases, the error decreases, as the clusters become smaller and more specific. In the SSE vs. k graph, we look for the point where there is a "break" or "bend" (an elbow). This point indicates that increasing k beyond it brings marginal gains in error reduction, signaling the optimal number of clusters.

We can see where the curve forms an elbow between values 2 and 3, with k=2 being slightly more accentuated in its curvature, suggesting the best value for K, but it's not very distinct for k=3.

###### Silhouette Index:
```{python}
silhouette_scores = []
K_range_sil = range(2, 11)
for k in K_range_sil:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(df_pca)
    score = silhouette_score(df_pca, labels)
    silhouette_scores.append(score)
fig_silhouette = go.Figure()
fig_silhouette.add_trace(
    go.Scatter(
        x=list(K_range_sil),
        y=silhouette_scores,
        mode='lines+markers',
        marker=dict(
            size=8,
            color='blue',
            symbol='circle'
        ),
        line=dict(
            width=2,
            color='blue'
        ),
        name='Silhouette Score'
    )
)

fig_silhouette.update_layout(
    title='Silhouette Index Analysis',
    xaxis_title='Number of Clusters (K)',
    yaxis_title='Silhouette Score',
    template='plotly_white',
    width=800,
    height=500
)

fig_silhouette.show()
```

###### Checking silhouette values:
```{python}
for k, score in zip(K_range_sil, silhouette_scores):
    print(f"K={k}: Silhouette Score={score:.4f}")
```

Interpretation: The values measure the quality of clusters by calculating how similar a point is to its own cluster compared to other clusters. The value ranges between -1 (poor grouping) and 1 (optimal grouping). Here we can visualize the best value for k=3, but very close for k=2.

### Results and Decision

**Elbow Method - K = 2**
The elbow shows where the reduction in inertia begins to stabilize.

Practical interpretation: The data may have two macro structures, meaning a coarser division.

**Silhouette - Best at K = 3**
The highest silhouette value (0.1239) occurs with K=3.

We will use k=2 and k=3 and analyze which algorithm best suits our project.

###### Applying KMeans Algorithm for K=2 and K=3:
```{python}
# Aplicar KMeans para k=2
kmeans_k2 = KMeans(n_clusters=2, random_state=42)
clusters_k2 = kmeans_k2.fit_predict(df_pca)
# DataFrame com clusters K=2
df_clusters_k2 = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(df_pca.shape[1])])
df_clusters_k2['Cluster'] = clusters_k2
# Aplicar KMeans para K=3
kmeans_k3 = KMeans(n_clusters=3, random_state=42)
clusters_k3 = kmeans_k3.fit_predict(df_pca)
# DataFrame com clusters K=3
df_clusters_k3 = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(df_pca.shape[1])])
df_clusters_k3['Cluster'] = clusters_k3
```

###### Quantitative Evaluation of the two models -- Silhouette Score:
```{python}
silhouette_k2 = silhouette_score(df_pca, clusters_k2)
silhouette_k3 = silhouette_score(df_pca, clusters_k3)
print(f'Silhouette Score K=2: {silhouette_k2}')
print(f'Silhouette Score K=3: {silhouette_k3}')
```

###### Cluster Distribution:
```{python}
## DistribuiÃ§Ã£o dos Clusters
print("\nCluster Distribution - K=2")
print(pd.Series(clusters_k2).value_counts())

print("\nCluster Distribution - K=3")
print(pd.Series(clusters_k3).value_counts())
```

###### Comparison between K=2 and K=3:
```{python}
fig = make_subplots(rows=1, cols=2, subplot_titles=('Clusters with K=2', 'Clusters with K=3'))

# Cores para K=2
unique_clusters_k2 = sorted(df_clusters_k2['Cluster'].unique())
if len(unique_clusters_k2) > 1:
    colors_k2 = pcolors.sample_colorscale("Viridis", np.linspace(0, 1, len(unique_clusters_k2)))
elif len(unique_clusters_k2) == 1:
    colors_k2 = [pcolors.sample_colorscale("Viridis", 0.5)[0]]
else:
    colors_k2 = []

for i, cluster_val in enumerate(unique_clusters_k2):
    df_subset = df_clusters_k2[df_clusters_k2['Cluster'] == cluster_val]
    fig.add_trace(go.Scatter(
        x=df_subset['PC1'],
        y=df_subset['PC2'],
        mode='markers',
        marker=dict(
            color=colors_k2[i],
            opacity=0.7,
            size=7
        ),
        name=f'K=2, Cluster {cluster_val}',
        legendgroup='k2_group'
    ), row=1, col=1)

# Cores fixas para K=3
cores_k3 = ['red', 'blue', 'green']
unique_clusters_k3 = sorted(df_clusters_k3['Cluster'].unique())
for i, cluster_val in enumerate(unique_clusters_k3):
    df_subset = df_clusters_k3[df_clusters_k3['Cluster'] == cluster_val]
    cor = cores_k3[i % len(cores_k3)]
    fig.add_trace(go.Scatter(
        x=df_subset['PC1'],
        y=df_subset['PC2'],
        mode='markers',
        marker=dict(
            color=cor,
            opacity=0.7,
            size=7
        ),
        name=f'K=3, Cluster {cluster_val}',
        legendgroup='k3_group'
    ), row=1, col=2)

# Ajustando eixos
fig.update_xaxes(title_text="PC1", row=1, col=1)
fig.update_yaxes(title_text="PC2", row=1, col=1)
fig.update_xaxes(title_text="PC1", row=1, col=2)
fig.update_yaxes(title_text="PC2", row=1, col=2)

# Layout ajustado
fig.update_layout(
    width=1100,
    height=550,
    hovermode='closest',
    showlegend=False,
    title={
        'text': "Cluster Distribution in space for K=1 and K=2",
        'y': 0.97,
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'pad': {'b': 25}
    },
    margin=dict(t=80, b=50, l=40, r=40),
    template='plotly_white'
)

fig.show()
```

###### Visualizing the center of each cluster:
```{python}
cluster_colors = ['red', 'blue', 'green']
fig = go.Figure()
unique_clusters = sorted(df_clusters_k3['Cluster'].unique())
for cluster_idx, cluster_num in enumerate(unique_clusters):
    mask = df_clusters_k3['Cluster'] == cluster_num
    fig.add_trace(go.Scatter(
        x=df_clusters_k3.loc[mask, df_clusters_k3.columns[0]],
        y=df_clusters_k3.loc[mask, df_clusters_k3.columns[1]],
        mode='markers',
        marker=dict(
            color=cluster_colors[cluster_idx % len(cluster_colors)],
            opacity=0.7,
            size=8
        ),
        name=f'Cluster {cluster_num}'
    ))
fig.add_trace(go.Scatter(
    x=kmeans_k3.cluster_centers_[:, 0],
    y=kmeans_k3.cluster_centers_[:, 1],
    mode='markers',
    marker=dict(
        size=16,
        color='black',
        symbol='x'
    ),
    name='Centroids'
))
fig.update_layout(
    title="Centroids of each Cluster with k=3",
    xaxis_title=df_clusters_k3.columns[0] if len(df_clusters_k3.columns) > 0 else 'Component 1',
    yaxis_title=df_clusters_k3.columns[1] if len(df_clusters_k3.columns) > 1 else 'Component 2',
    legend_title_text='Legend',
    width=900,
    height=650,
)
fig.show()
```

### Choice of K=3

Although the Silhouette metric is only slightly superior for K=3 (0.1239) compared to K=2 (0.1198), it still suggests that the model with three clusters offers a more refined division of respondents' behavioral profiles.

The 2D PCA plots show some overlap between clusters (consistent with low silhouette scores) but also reveal that the cluster centers are in distinct positions, indicating that K-Means successfully found different patterns and that the groups capture relevant differences in behavioral profiles.

## Cluster Interpretation and Insight Generation

###### Getting and visualizing the centroids:
```{python}
centroids_pca = kmeans_k3.cluster_centers_
centroids_ohe = pca.inverse_transform(centroids_pca)
centroids_df = pd.DataFrame(centroids_ohe, columns=ohe.get_feature_names_out())
print(centroids_df)
```

###### Frequency table of answers in each Cluster:
```{python}
# Junta o cluster ao dataframe original
df_clusters = df_gf.copy()
df_clusters['Cluster'] = clusters_k3

titulos_perguntas = {
    'pergunta_1': 'Clarity Level',
    'pergunta_2': 'Current Financial Situation',
    'pergunta_3': 'Support and Involvement',
    'pergunta_4': 'Planning Organization Level',
    'pergunta_5': 'Current Investment Possibility',
    'pergunta_6': 'Desired Wedding Style',
    'pergunta_7': 'Honeymoon Planning',
    'pergunta_8': 'Commitment to make it real'
}

perguntas = df_gf.columns.tolist()
num_perguntas = len(perguntas)
n_rows = 3
n_cols = 3
subplot_titles_list = []
for i in range(n_rows * n_cols):
    if i < num_perguntas:
        pergunta_nome = perguntas[i]
        titulo = titulos_perguntas.get(pergunta_nome, pergunta_nome)
        subplot_titles_list.append(f'{titulo}')
    else:
        subplot_titles_list.append('')

fig = make_subplots(
    rows=n_rows,
    cols=n_cols,
    subplot_titles=subplot_titles_list,
    horizontal_spacing=0.07,
    vertical_spacing=0.12
)

palette = qualitative.Vivid
unique_cluster_values = sorted(df_clusters['Cluster'].unique())
cluster_color_map = {
    cluster_val: palette[i % len(palette)]
    for i, cluster_val in enumerate(unique_cluster_values)
}

for i, pergunta in enumerate(perguntas):
    if i >= n_rows * n_cols:
        break
    row_num = (i // n_cols) + 1
    col_num = (i % n_cols) + 1

    ordem_categorias = df_clusters[pergunta].value_counts().index.tolist()

    for cluster_val in unique_cluster_values:
        df_subset_cluster = df_clusters[df_clusters['Cluster'] == cluster_val]
        counts = df_subset_cluster[pergunta].value_counts()
        y_values = [counts.get(cat, 0) for cat in ordem_categorias]

        fig.add_trace(go.Bar(
            x=ordem_categorias,
            y=y_values,
            name=f'Cluster {cluster_val}',
            marker_color=cluster_color_map[cluster_val],
            showlegend=False
        ), row=row_num, col=col_num)

    fig.update_xaxes(
        type='category',
        categoryorder='array',
        categoryarray=ordem_categorias,
        tickangle=0,
        showticklabels=True,
        row=row_num,
        col=col_num
    )
    fig.update_yaxes(
        title_text=None,
        row=row_num,
        col=col_num
    )

fig.update_layout(
    height=1100,
    width=1100,
    barmode='group',
    template='plotly_white',
    showlegend=False,
    title={
        'text': 'Frequency of answers in each Cluster',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'y': 0.97,
        'pad': {'b': 20}
    },
    margin=dict(t=100, b=65, l=35, r=35)
)

fig.show()
```

###### Calculating the top 5 averages for each answer per cluster:
```{python}
df_encoded = pd.get_dummies(df_clusters.drop('Cluster', axis=1), prefix_sep='_', drop_first=False)
df_encoded['Cluster'] = df_clusters['Cluster']
centroids = df_encoded.groupby('Cluster').mean()

fig = make_subplots(
    rows=1, 
    cols=3, 
    subplot_titles=[f"Cluster {i}" for i in centroids.index],
    specs=[[{"type": "table"}]*3]
)

for i, cluster in enumerate(centroids.index):
    top5 = centroids.loc[cluster].sort_values(ascending=False).head(5)

    fig.add_trace(
        go.Table(
            header=dict(
                values=["Answer", "Proportion"],
                fill_color="lightgrey",
                align="left",
                font=dict(color="black", size=12)
            ),
            cells=dict(
                values=[top5.index, top5.values.round(3)],
                align="left",
                height=30
            )
        ),
        row=1, col=i+1
    )

fig.update_layout(
    height=300, 
    width=1000,
    template="plotly_white",
    title={
        'text': 'Most representative answers per Cluster',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'y': 0.96,
        'pad': {'b': 15}
    },
    margin=dict(t=70, b=35, l=30, r=30)
)
fig.show()
```

Based on the analysis of the centroids, which represent the average values (choice proportions) of each answer within each cluster, we can interpret that:

- The closer to 1, the more predominant this characteristic is in the group.

- Each value ranges from 0 to 1 and represents the relative frequency with which that option was chosen within the cluster.

This method allows us to understand the average profile and priorities of each segment.

## Cluster Interpretation

### Group 1: 234 leads (Cluster 1 â€” 47%)

##### Profile:

* Wedding Style (Question 6): Predominantly, they desire "Something intimate and simple, only with close people" (ðŸ”¸ pergunta_6_B â€“ 99.6%). This is the most striking trait of this group.
* Investment Possibility (Question 5): Most believe they "Could do something simple" if they were to have the wedding today (ðŸ”¸ pergunta_5_B â€“ 62.4%).
* Organization Level (Question 4): A significant portion "Haven't started yet" planning (ðŸ”¸ pergunta_4_A â€“ 57.3%).
* Honeymoon Planning (Question 7): Most "Haven't thought about it yet" (ðŸ”¸ pergunta_7_A â€“ 56.8%).
* Partner's Support (Question 3): The partner is "Completely involved, dreaming with me" (ðŸ”¸ pergunta_3_A â€“ 51.7%).

##### Behavior Summary:

* This is the largest group and is characterized by a clear desire for a simpler, more intimate wedding.
* Financially, they feel capable of holding a modest event, but have not yet started practical organization or honeymoon planning.
* Partner involvement is high, indicating a shared dream.
* Despite the desired simplicity, the lack of initiation in planning suggests a need for guidance to take the first steps, even for a smaller event.

##### Needs:

* Ideas and inspirations for simple, elegant, and economical weddings.
* Planning tools focused on smaller, more objective events.
* Direction on how to start planning an intimate wedding without complications.
* Content that validates the choice for a smaller wedding, showing its benefits and charm.

---

### Group 2: 206 leads (Cluster 0 - 42%)

##### Profile:

* Wedding Style (Question 6): They desire "A charming ceremony, with everything well done", but not necessarily the most luxurious (ðŸ”¸ pergunta_6_C â€“ 62.6%).
* Partner's Support (Question 3): The partner is "Completely involved, dreaming with me" (ðŸ”¸ pergunta_3_A â€“ 53.4%).
* Honeymoon Planning (Question 7): The vast majority "Haven't thought about it yet" (ðŸ”¸ pergunta_7_A â€“ 49.0%).
* Investment Possibility (Question 5): They feel they "Couldn't afford anything yet" if the wedding were today (ðŸ”¸ pergunta_5_A â€“ 46.1%).
* Organization Level (Question 4): They "Haven't started yet" planning (ðŸ”¸ pergunta_4_A â€“ 44.7%).

##### Behavior Summary:
* This group, one of the largest, is at a very early stage. They have clear dreams and desires about the ceremony style and have strong mutual support as a couple.
* However, they face practical paralysis due to lack of organization and, crucially, the perception of financial incapacity at the moment.
* They are dreaming big, but feel lost about where to start, with budget and organization being the main bottlenecks.

##### Needs:
* Practical, step-by-step guides: "From scratch to dream wedding: a beginner's guide".
* Basic organization tools: simple checklists, initial timelines, budget spreadsheet templates for beginners.
* Solutions and ideas for affordable weddings: Content on how to achieve a charming ceremony on a limited budget.
* Emotional and motivational content: Reinforcing that it's normal to feel lost at the beginning and that it's possible to turn the dream into reality with planning, even with limited resources.

---

### Group 3: 55 leads (Cluster 2 â€” 11%)

##### Profile:

* Clarity Level (Question 1): They have a very high level of clarity: "We know exactly what we want and have already started organizing" (ï¿½ï¿½ pergunta_1_D â€“ 89.1%).
* Commitment (Question 8): They are highly committed: "We are ready, we want to act and truly achieve it" (ðŸ”¸ pergunta_8_D â€“ 85.5%).
* Organization Level (Question 4): They are already well organized: "We have spreadsheets, goals, and even a defined timeline" (ðŸ”¸ pergunta_4_D â€“ 58.2%).
* Investment Possibility (Question 5): They believe they "Could afford a good part, but we want more freedom" financially (ðŸ”¸ pergunta_5_D â€“ 49.1%).
* Partner's Support (Question 3): The partner is "Completely involved, dreaming with me" (ðŸ”¸ pergunta_3_A â€“ 47.3%).

##### Behavior Summary:

* This is the smallest group, but it represents the most decisive and proactive brides.
* They have total clarity about the desired wedding, are highly committed, and already have advanced planning.
* Financially, they are in a relatively comfortable position, but seek to optimize their resources for "more freedom."
* Partner support is also strong, indicating a joint and aligned effort.
* They have probably researched extensively and may be looking to optimize what they have already planned or find suppliers and solutions that fit their clear vision.

##### Needs:

* Solutions to optimize the budget and maximize the value of the investment.
* Advanced tools for vendor management or detailed timelines.
* Specialized consulting to refine details or resolve specific planning points.
* Inspiration for finishing touches or differentiated elements that add value to the already well-defined wedding.
* Confirmation that they are on the right track and access to trusted suppliers.

###### Lead counts in each Group:
```{python}
# Contagem dos clusters
cluster_counts = df_clusters['Cluster'].value_counts().sort_index()
x_labels = {0: 'Grupo 2', 1: 'Grupo 1', 2: 'Grupo 3'}
x_axis_labels = [x_labels[c] for c in cluster_counts.index]
vivid_palette_px = px.colors.qualitative.Vivid
bar_colors = [vivid_palette_px[i % len(vivid_palette_px)] for i in range(len(cluster_counts.index))]
fig = go.Figure()

fig.add_trace(go.Bar(
    x=x_axis_labels,
    y=cluster_counts.values,
    marker_color=bar_colors,
    text=cluster_counts.values,
    texttemplate='<b>%{y}</b>',
    textposition='outside',
    textfont=dict(size=12, color='black', family='Arial')
))

fig.update_layout(
    title=dict(
        text='Leads by Group',
        font=dict(size=20, color='black', family='Arial Black'),
        x=0.5,
        xanchor='center',
        y=0.97,
        yanchor='top',
        pad={'b': 12}
    ),
    xaxis_title=None,
    xaxis=dict(
        tickangle=0,
        type='category',
        tickfont=dict(size=14, color='black')
    ),
    yaxis=dict(
        title=dict(
            text='NÃºmero de Leads',
            font=dict(size=14)
        ),
        showgrid=True,
        gridcolor='rgba(211, 211, 211, 0.7)',
        griddash='dash',
        gridwidth=1,
        range=[0, cluster_counts.values.max() * 1.15]
    ),
    width=520,
    height=450,
    plot_bgcolor='white',
    font=dict(size=14),
    margin=dict(t=75, b=55, l=60, r=60)
)

fig.show()
```

###  **Segments Summary**

| Cluster |  Leads |        Style          | 	Organization   |    Investment    |     Emotion/Commitment      |
| ------- | ------ | --------------------- | ----------------- | ---------------- | --------------------------- |
| Group 1 |  234   | Simple and intimate   | Loose ideas       | Something simple | Dreaming together, cautious |
| Group 2 |  206   | Charming, but lost    | Haven't started   | Cannot afford    | Dreaming, but lost          |
| Group 3 |  55    | Charming to grandiose | Extremely high    | Can afford well  | Very high commitment        |

The proposed segmentation clearly shows three very distinct profiles, with different needs, desires, and conditions.

The use of KMeans with K=3 was the most appropriate, as it captured:

* Two large groups focusing on simplicity, but with subtle differences in the degree of organization and insecurity.

* A smaller, but very valuable group of high-conversion clients and higher average ticket.